[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Socio-economic analysis with Python",
    "section": "",
    "text": "Preface",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#what",
    "href": "index.html#what",
    "title": "Socio-economic analysis with Python",
    "section": "What",
    "text": "What\nThis syllabus provides the course material for the first three weeks of the course Sociaaleconomische Analyse (in Dutch). With this course we would like to bridge the gap between on the one hand applied statistics and (micro-economic) modeling and on the other hand putting all this in practice when performing empirical research. As such this course can as well be seen as preparation for the bachelor thesis. But above all, the course aims to provide students with some tools that we see as very useful for research; not only in the socio-economic sciences but outside them as well.\nAs we only have a limited amount of time available for this course, the amount of topics we can deal with is by its nature restricted. We decided to focus in the first three week on the basics of applied econometrics and as such this first part builds upon the foundations of the statistics course in the first period of the second year. But now we challenge the student to build more elaborate statistical models where specific attention is given to presentation and interpretation of the results. The last three parts of the course move on to economic welfare from a behavioral perspective. Not only is the concept of economic welfare central to almost all economic theories, the behavioral perspective allows for more reflection on the neo-classical assumptions typically made in introductionary economic courses.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#why",
    "href": "index.html#why",
    "title": "Socio-economic analysis with Python",
    "section": "Why",
    "text": "Why\nAlthough there are many and very good introductory textbooks on economic models and applied econometrics, the combination of the two is seldom seen. Apart from that there are two reasons why we wanted to write our own material. First, usually less time is spent on why certain, and at first sight very restrictive, assumptions are made. We want to bridge that gap and provide the student with more intuition on where models, evidence, and finally perhaps the “truth” (if there is such a thing) comes from. Second, how to present statistical evidence and the interpretation of that evidence is very important but usually not given much attention.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#for-whom",
    "href": "index.html#for-whom",
    "title": "Socio-economic analysis with Python",
    "section": "For Whom",
    "text": "For Whom\nThis syllabus assumes that the reader has a basic working knowledge of statistics, data science and some calculus (typically those method courses Earth, Economics and Sustainability students enjoy in their first year and in period I of the second year). The syllabus can however be read as stand-alone, although that requires some more attentive reading and practising. Where we think it is necessary we provide (references to) background material. For the course Sociaaleconomische analyse both types of syllabi should be read in total and it might be wise to read the relevant material before the respective lecture. The big advantage of course is that lectures and reading material now really go one to one.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Packages used in this chapter",
    "section": "",
    "text": "2 Introduction\nIn this first introductory chapter, I will lay out the relation between theory development and theory testing as they are the cornerstones of scientific progress. After all, a theory or idea can only be scientific if the theory can be tested and, if need be, refuted. If the theory cannot be tested then it is not science. I will also explain the basic workflow of scientific research and the tools needed with specific emphasis on research in the social sciences. This chapter ends with a reading guide where we discuss each chapter in this syllabus and the relations between the chapters.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Packages used in this chapter</span>"
    ]
  },
  {
    "objectID": "intro.html#theory-models-and-hypotheses",
    "href": "intro.html#theory-models-and-hypotheses",
    "title": "1  Packages used in this chapter",
    "section": "2.1 Theory, Models and Hypotheses",
    "text": "2.1 Theory, Models and Hypotheses\nIn 2021, Guido Imbens, Joshua Angrist and David Card received the Nobel price for economics (officially The Sveriges Riksbank Prize in Economic Sciences in Memory of Alfred Nobel). The field they work in is applied econometrics with specific focus on finding causal relations. That means that with data they want to test whether phenomenon \\(X\\) has an effect on phenomenon \\(Y\\). More, in detail, with a causal effect we mean that when we change \\(X\\), there will be an effect on \\(Y\\), ceteris paribus, and not necessarily the other way. So, when we change \\(Y\\), \\(X\\) will not necessarily change.\nAnd identifying causal effects is what most applied econometric work nowadays is focused on. And we will focus on that as well in ?sec-univariateregression, ?sec-modeling and ?sec-specification. But how do you know what to test, or, in other words, where do phenomena \\(X\\) and \\(Y\\) come from? Those phenomena and possible relations originate from scientific theories as you will have in all disciplines. And those theories are typically casts in models—usually in a very abstract manner. Models come in the form of computer simulations (such as with agent-based modeling), real physical models (as with displays), but often models are formulated in mathematical notation with the aim of being as precise, lucid and clear as possible. But note that these models are not necessarily theory. Theory is the underlying set of relations and assumptions that can say something about the specific structure of models. But very often one theory can lead to multiple models, each perhaps highlighting different aspects of the underlying theory. An example of such a theory is the Law of Diminishing Marginal Utility: each additional unit of the same good is appreciated less by consumers. This theory can be expressed in many mathematical ways but the underlying concept as displayed in Figure 2.1 always remains the same. This type of function, increasing but slower and slower, belongs to the family of concave functions. A function with exponential growth (such as \\(e^{gt}\\) belongs to the family of convex functions). But how such a function should exactly be defined is not a-priori clear.\n\n\n\n\n\n\n\n\nFigure 2.1: Law of Diminishing Marginal Utility\n\n\n\n\n\nSo how to relate this with each other in scientific research? Well, when doing research you are interested in something that is not yet known (the research gap). Your aim is to (partly) fill this research gap by answering a research question. To answer this research question you need theory (a theoretical framework); what do you need to assume, what are the most important (moderator) variables, how do they relate with each other, and so on and so forth. From this theory you construct a model. Not necessarily a mathematical one. For example, you can also make a model in a Geographical Information System environment where you visualize layers of information that you think are most relevant based upon theory (in this case often previous scientific literature). Or you make a simulation model examining risks of flooding by rivers. The final step is the stage where your model should provide you with some answers. Sometimes they are concerned with optimality (what is the best location of a new road in a GIS environment), prediction (where are river dikes most vulnerable), or with establishing a (causal) relation. And it is the latter that this course deals with. How can we know that there is a relation between phenomenon \\(X\\) and phenomenon \\(Y\\) and how do we know whether that relation is causal?\nFor that we use applied econometrics (which is a form of applied statistics but then in the social-economic sciences domain—the exact difference will be discussed in ?sec-univariateregression). And to establish a, hopefully causal, relation, we test our models with empirical data. Be aware, though, that the applied econometrics materials we teach in this course (and in all introductory courses of applied statistics and econometrics all over the world—the “101” courses) is based on so-called frequentist statistics (To freshen up you knowledge about the basics of statistics you might want to read Appendix A). The exact definition is not important for now but know that it is intrinsically related with hypothesis testing.\nAnd hypothesis testing is most often associated with that scientific philosopher—and perhaps the only one you know—Karl Popper (as displayed in Figure 2.2).\n\n\n\n\n\n\n\n\nFigure 2.2: Karl Popper\n\n\n\n\n\nPopper was a so-called empiricist and claimed that theories in the empirical sciences (that includes most of the social sciences) can never be proven, only rejected. That is why you can reject a null-hypothesis (\\(H_0\\)), but never accept the alternative hypothesis (\\(H_a\\)). And this is highly related with the theoretical framework behind frequentist statistics. Loosely speaking, in frequentist statistics you construct a world where \\(H_0\\) is true and you try to reject that world with data (we will come back to this in ?sec-univariateregression)—but that world does not say anything about the validity of the alternative hypothesis. Now, Popper never claimed that rejecting one null-hypothesis will reject a whole theory. For that you need a larger body of evidence, including results from all sorts of studies—not only statistical ones, leading to a general consensus amongst the whole scientific community (Popper 2005).\nIn truth, although the scientific approach of working with null-hypotheses is a very valuable one (and remarkably practical), it does not always lead to definitive answers. That is because contradicting models can lead to similar null-hypotheses. Moreover, often the connection between research question and null-hypothesis is not a direct one. Consider that you want to know the effect of \\(X\\) on \\(Y\\), so your research question is: “What is the effect of \\(X\\) on \\(Y\\)?”. But, in a frequentist world you only reject hypotheses—thus leading to results in the line of: “The effect of \\(X\\) on \\(Y\\) is not \\(\\ldots\\)”. This makes the evidence for your research question at least circumstantial and in the best case indirect. The bottom-line here is that one needs to be careful in drawing conclusions based on null-hypotheses (and in a broader sense based on models in general). Scientific research typically advances very slowly—but hopefully in a robust and parsimonious way!",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Packages used in this chapter</span>"
    ]
  },
  {
    "objectID": "intro.html#doing-research-in-the-social-sciences",
    "href": "intro.html#doing-research-in-the-social-sciences",
    "title": "1  Packages used in this chapter",
    "section": "2.2 Doing Research (in the Social Sciences)",
    "text": "2.2 Doing Research (in the Social Sciences)\nIt is remarkable that, although in principle students are (should be) prepared for scientific research, they receive little guidance in how they should do scientific research. What are the tips & tricks of the trade and what—and, more importantly why—should you use with respect to specific (types of) applications and what is the relation between them. In our view most of this should belong in your first course upon entering the university (with the appropriate course title “Research Methods 101”). And some of it you indeed have learned in your first year, but in our experience students still lack “operational” knowledge. Therefore, we discuss below the four elements we think are among the most important—at least for this course. There are others, but for now this will do.\n\n2.2.1 Work tidy\nOur first and most important tip is to work tidy. Try to make your work look good. And with work we mean everything you submit (such as tutorials, papers, examinations, and theses). And that is because lecturers are just like people and often think from primary instincts with their reptilian brain: if it doesn’t look good, not much time is spent on arguing and thinking as well! Moreover, when your work is difficult to read, lecturers get annoyed. Making your work look good and in the same time more lucid and transparent also serves a higher purpose as it is then easier to detect mistakes. Namely, everyone makes mistakes. The important thing is to detect them early, learn from them and remedy them. This advances science in general and is a very important feature of the scientific process. ?sec-specification will spent additional time on working tidy and making it looking good.\n\n\n2.2.2 Know where your stuff is\nA second very straightforward tip is to be organised and to know where your stuff is. Often, students come to us for help with all their files piled on a stack on their desktop and facing difficulties finding where their work is. It is always advisable to use a folder structure and have one folder for one project (or for one course). And to the use sub-folders for data, text, code, pdf’s and so forth. A second tip for organisation is to think about versioning. As the well-known Figure Figure 2.3 shows the number of versions of one file very quickly can get out of hand. Think at least about a consistent naming structure (perhaps with the date involved such as paper_20221215.doc).\n\n\n\n\n\n\n\n\nFigure 2.3: Version confusion\n\n\n\n\n\n\n\n2.2.3 Make notes\nOne skill that in our opinion is given not too much attention is making useful notes. It has been proven that writing things down is beneficial; not only for remembering but also for understanding. And that seems to be best just by using a pen as this slows writing down and you have to think about what to write down. Underlining or marking is useful less beneficial than writing accompanying notes. But when should you write notes? Well, when attending lectures of course but also when reading. To leverage your notes as much as possible it is important that you have a system where you can retrieve your notes and compare them with other notes. The latter is the hardest part, but is in the long run the most rewarding as new connections are created between lectures, courses, books, and years. You do not need any fancy tools for this (there is literally a ton of applications to be found on internet), Microsoft’s Onenote or Evernote are more than good enough. Where the workflow typically is to first use pen and paper to capture notes and thereafter rewrite and organise your notes in a notes system.\n\n\n2.2.4 Use a reference manager!\nPerhaps the tool that has the quickest pay-off is a reference manager. For those of you who are not using one yet: do it. Why? Because you never have to think about your reference list again. All reference managers come with plugins for Word or other text-editors (or type-setters such as LaTeX that enable you to automatically generate reference list based upon in-text citations which the reference manager can also provide. You only need less than an hour to set it up, but you very quickly become more efficient (and thus save time in future work). There are many reference managers out there, but we advise Zotero as it is open source. There is both a cloud and desktop version and it comes with a handy tutorial. It also provides a plugin for your browser to automatically import the bibliographic details of the paper you are reading at the moment.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Packages used in this chapter</span>"
    ]
  },
  {
    "objectID": "intro.html#statistical-software-python",
    "href": "intro.html#statistical-software-python",
    "title": "1  Packages used in this chapter",
    "section": "2.3 Statistical software Python",
    "text": "2.3 Statistical software Python\nAs quantitative research becomes more and more important in the social sciences you need software to manage your data and provides statistical and applied econometric analyses. We will use Python in this course. However, note that Python has a steeper learning curve than e.g., STATA, and does not work immediately out-of-the-box. But the user base is large and that is important, because for each problem there is much material to be found on internet (including videos). In this syllabus we will give specific attention to Python-scripts, including why you code it like such and what the intuition is behind that.\n\n2.3.1 Where to get Python\nFirst, you need to install R itself. You can do this by downloading this from CRAN (choose the server you want). Now choose your appropriate operating system, choose the base system, download R and install it. That’s it!\nThe base distribution of R comes with a built-in editor, where you can write your script. This editor is however very basic. Therefore, it is very much advised that you download and install the free editor RStudio as well. Again, choose your operating system and just install the latest version. The very short video (6 minutes) website gives an overview of the basic features (it can do quite some more stuff).\n\n\n2.3.2 Why use Python and not any other application?\nAsk any data scientist at the moment for the software tools most used and they will most likely answer R or Python. Of course, that should not be a valid answer (many people use Word as well and nobody would argue that Word is brilliantly programmed or designed), but it indicates the popularity (and the community) that uses Python.\nWhere 15 years ago most social scientists still used SPSS (and the economists Stata), that has now changed completely (well, the economists still use Stata, but the rest of the world moved on). And for good reasons, namely:\n\nIt is open source and thus free;\nPython is flexible and thus multi-purpose;\nthere is now a very large userbase; everything you can dream of (that is, in the context of data science/management), somebody else most likely already programmed; \nit generates beautiful pictures, diagram, maps, and histograms (even 3D pie diagrams for the masochists amongst us);\nrelative to Stata or Excel it is fast, which is great for larger (spatial) databases.\n\nCRAN packages give a great overview of all the official packages out there and the wide range of applications, and again they are all free!In general, you can use Python for statistical analysis, simulation analysis, data management, visual display of data, creating documents (and presentations), and even GIS applications. In that respect it is far more flexible than Stata. Last but perhaps not least, Python is more and more used outside academia as well. Twitter, Facebook, Booking and Google use Python.\nSocial science students might find working with Python initially strange, cumbersome or even frustrating. All the lovely drop-menus that are still provided by Excel and Stata have disappeared, and the whole thing is completely script-driven. In fact, Python is a full-blown program language. I am aware that this needs some adaptation. However, hopefully, learning Python in combination with Jupyter-notebooks will pay-off; if not in becoming more efficient and reproducable, then at least in the fact that you start to understand a different way of doing things and that the office suite (Word, Excel and Powerpoint) is not the only option out there.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Packages used in this chapter</span>"
    ]
  },
  {
    "objectID": "intro.html#reading-guide",
    "href": "intro.html#reading-guide",
    "title": "1  Packages used in this chapter",
    "section": "2.4 Reading Guide",
    "text": "2.4 Reading Guide\nThis course will not concern itself with theory as such, but more with how to test that theory (the applied in applied econometrics). ?sec-univariateregression introduces the basic concepts of applied econometrics in the form of univariate regression. ?sec-modeling extends this framework to a multivariate regression setting, but in the same deals as well with the translation of theoretical (socio-economic) models to empirical models that are testable. ?sec-specification discusses how to specify your model—which variables should you include and which variables not—and how to present your findings to a wider audience (that includes assessors). The final chapter summarizes and provides a general discussion.\n\n\n\n\nPopper, Karl. 2005. The Logic of Scientific Discovery. Routledge.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Packages used in this chapter</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Adams, Douglas. 1995. Hitchhiker’s Guide to the Galaxy (Book\n1). New York: Del Rey Bonks.\n\n\nPopper, Karl. 2005. The Logic of Scientific Discovery.\nRoutledge.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "appendix.html",
    "href": "appendix.html",
    "title": "Appendix A — Reviewing probability and statistics",
    "section": "",
    "text": "A.1 Reviewing probability",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Reviewing probability and statistics</span>"
    ]
  },
  {
    "objectID": "appendix.html#reviewing-probability",
    "href": "appendix.html#reviewing-probability",
    "title": "Appendix A — Reviewing probability and statistics",
    "section": "",
    "text": "A.1.1 Probability\nAs a definition of probability we use the concept of empirical probability which is the proportion of time that something (a specific outcome or event \\(X\\)) occurs in the long-run of total events. Usually it is give by:\n\\[\\begin{equation}\n\\text{probability} = p = \\frac{\\text{Number of times specific event $X$ happens}}{\\text{Total amount of events that can happen}}\n\\end{equation}\\]\nNow probabilities are defined by a set of definations (axioms). These are:\n\nProbabilities, \\(p\\), are always between 0 and 1. So, \\(0 \\leq p \\leq 1\\)\nIf something does not happen, then \\(p = 0\\)\nIf something always happens, then \\(p = 1\\)\nProbabilities for the total amount of events always add up to \\(1\\). So, if the probability that something happens is \\(p\\), then the probabilities that it will not happen is \\(1 -p\\) (see that \\(p + 1 - p = 1\\))\n\n\n\nA.1.2 Population & random variables\nIn general we see a population as the the group or collection of all possible entities of interest (school districts, inhabitants of the Netherlands, homeowners) and we will think of populations as infinitely large (\\(\\infty\\)). From this population we then sample specific observations. This sample contains then a random variable \\(Y\\), which denotes a characteristics of the entity (district average test score, prices of houses, prices of meat). An important feature is that the specific contents of the sample is unknown, that is before measurement (\\(y\\)), after measurement the sample is know and is called data.\nSo, a random variable (also called a stochastic variable) is a mathematical formalization of something that depends on random outcomes. Unfortunately, randomness is not clearly defined and depends on specific scientific philosophical schools. The scientific philosophical school we implicitly assume in this course—and, in fact, in most statistical courses—is that of frequentist statistics. Here we assume that all things we measure are intrinsically random. In fact, this is an ontological argument—in other words, what are our beliefs in the state of the world. Because all things we measure are random, every time we measure something our measurements are (slightly) different. However, the more we measure, the more precise we know something. But there is still randomness.\nIn general, there are two types of random variables. First, there are discrete random variables, where outcomes can be counted, such as \\(0, 1, 2, 3, \\ldots\\) and continuous random variables, where outcomes can be any real number.1\n\n\nA.1.3 Distribution functions\nRandom variables are governed by distribution functions which are mathematical functions that provides the probabilities of occurrence of all different possible outcomes of a specific experiment2: e.g. for a discrete distribution, \\(f(x) = \\Pr(Y = y)\\) \\(\\forall y\\). Or, in other words, the distribution function maps discrete outcomes to probabilities. For continuous distribution function, this is not possible as there an infinite number of possible outcomes, so that means that for each specific \\(y\\) must yield \\(\\Pr(Y = y) = 0\\). Therefore, with continuous distributions, often the cumulative distribution function is used, which is defined as \\(F(x) = \\Pr(Y \\leq y)\\). This is why we always use the surface of areas under the normal distribution function.\nDistribution functions have characteristics of which the most important are:\n\nThe mean, also known as the expected value (or expectation) of \\(Y\\). It is usually denoted as \\(E(Y) = \\mu_Y\\) and can as well be interpreted as the long-run average value of \\(Y\\) over repeated realizations of \\(Y\\): \\(\\frac{1}{n}\\sum_{i = 1}^{n}y_{i}\\)\nThe variance, which is denoted as \\(E(Y - \\mu_Y)^2\\). Usually it is associated with the symbol \\(\\sigma^2_Y\\) and provides a measure of the squared spread of the distribution. If we take the square root then we have the standard deviation (\\(=\\sqrt{\\text{variance}} = \\sigma_Y\\)). For a symmetrical normal distribution, it is useful to know that the mean plus or minus 1 time the standard deviation governs about \\(2/3\\) of all probability while the mean plus or minus 2 times the standard deviation governs about 95% of all probability associated with that random variable.\n\nNow, in statistics we are usually related in relations between random variables, and luckily most entities in real life are related. To capture that relation we need two concepts, joint distributions and covariance. If we assume that that random variables \\(X\\) and \\(Z\\) have a joint distribution then the covariance between \\(X\\) and \\(Z\\) is: \\[\\begin{equation}\ncov(X,Z) = E[(X- \\mu_X)(Z- \\mu_Z)] = \\sigma_{XZ}\n\\end{equation}\\]\nNote that this covariance is a measure of the linear association between \\(X\\) and \\(Z\\) and that its units are units of \\(X\\) times units of \\(Z\\). \\(cov(X,Z) &gt; 0\\) means a positive relation between \\(X\\) and \\(Z\\), and finally if \\(X\\) and \\(Z\\) are independently distributed, then \\(cov(X,Z) = 0\\). Note that the covariance of a random variable with itself is just its variance: \\[\\begin{equation}\ncov(X,X) = E[(X-\\mu_X)(X - \\mu_X)] = E[(X - \\mu_X)^2] = \\sigma^2_X\n\\end{equation}\\]\nHowever, the covariance is still measured in the units of \\(X\\) and \\(Z\\). To correct for that, we often use the correlation coefficient, defined by: \\[\\begin{equation}\ncorr(X,Z) = \\frac{cov(X,Z)}{\\sqrt{var(X)var(Z)}} = \\frac{\\sigma_{XZ}}{\\sigma{_X}\\sigma{_Z}} = r_{XZ}\n\\end{equation}\\] where \\(-1 \\leq corr(X,Z) \\leq 1\\), a \\(corr(X,Z) = 1\\) means perfect positive linear association, a \\(corr(X,Z) = -1\\) means perfect negative linear association, and a \\(corr(X,Z) = 0\\) denotes no linear association.\n\n\n\n\n\n\n\n\nFigure A.1: The correlation coefficient and the relation between observed \\(x\\) and \\(y\\)\n\n\n\n\n\nIt is very important to notice that a correlation coefficient measures linear association. So, \\(corr(X,Z) = 0\\) does not mean that there is no relation, there is only no linear correlation. This is illustrated by Figure A.1. In panel (a) there is clearly a positive relation, and panel (b) shows a negative relation, but what about panel (d)? Here, the correlation coefficient is 0, just as in panel (c), but obviously there is a clear non-linear relation.\n\n\nA.1.4 Conditional distributions and conditional means\nAn important notion in applied statistics (and in applied econometrics) is that of the conditional distribution, that is the distribution of \\(Y\\), given value(s) of some other random variable, \\(X\\). For example, in our California school example, we might want to know something about the distribution of test scores, given that \\(STR &lt; 20\\). Therefore, we use the concept of conditional mean, which is defined as the mean of a conditional distribution = \\(E(Y\\mid X = x)\\). Note here the \\(\\mid\\) symbol—it means the expected value of \\(Y\\) given that a random variable \\(X\\) is measured with \\(x\\). As an example: \\(E(Test scores \\mid STR &lt; 20)\\) which denotes the mean of test scores among districts with small class sizes. We also denote this with the conditional mean.\nNow, if we want to know the difference in means, then we can denote that with \\[\\begin{equation}\n\\Delta = E(Test scores \\mid STR &lt; 20)-  E(Test scores \\mid STR \\geq 20),\n\\end{equation}\\] which is a very important concept in applied economics as it resembles two groups of which one received treatment and the other one not. Other examples of the use of conditional means: difference in wages among gender (in the possible case of a glass ceiling for females) and mortality rate differences between those who are treated and those who are. Now if \\(E(X \\mid Z)\\) is constant, then \\(corr(X,Z) = 0\\). We then say that \\(X\\) and \\(Z\\) are independent.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Reviewing probability and statistics</span>"
    ]
  },
  {
    "objectID": "appendix.html#sec-sampling",
    "href": "appendix.html#sec-sampling",
    "title": "Appendix A — Reviewing probability and statistics",
    "section": "A.2 Sampling in frequentist statistics",
    "text": "A.2 Sampling in frequentist statistics\nSo, we mentioned above that we sample from the population which is assumed to be infinitely large. Now, how does this sampling then carry over to statistics. For that we need a statistical framework based on random sampling. First, choose an individual, \\(i\\), (or district, firm, etc.) at random from the population. Now, prior to sample selection, the value of what we want to know \\(Y_i\\) is random because the individual is randomly selected. Once the individual is selected and the value of \\(Y\\) is observed, then \\(Y\\) is just a number—not random anymore but data. And then we say it has the value \\(y\\). Hence the notation \\(\\Pr(Y = y\\)).\nIf we sample multiple entities, then we can construct a data set that looks like \\((y_1, y_2,\\dots, y_n)\\), where \\(y_i\\) = value of \\(y\\) for the \\(i^{\\mathrm{th}}\\) individual (district, entity) sampled. Again the lower case here denotes a realisation—the dataset. Now, we want to know what the distribution of the random variables \\(Y_1, \\ldots, Y_n\\) is under simple random sampling. Note that because entities (say individuals) #1 and #2 are selected at random, the value of \\(Y_1\\) has no information content for \\(Y_2\\). Thus: \\(Y_1\\) and \\(Y_2\\) are independently distributed. And if \\(Y_1\\) and \\(Y_2\\) come from the same distribution, that is, \\(Y_1\\), \\(Y_2\\) are identically distributed, then we say that, under simple random sampling, \\(Y_1\\) and \\(Y_2\\) are independently and identically distributed (i.i.d.). More generally, under simple random sampling, \\(Y_i\\), \\(i = 1,\\ldots, n\\), are i.i.d—this term always come back in all sorts of statistics.\nThis simple framework already allows rigorous statistical inferences about, e.g., the mean \\(\\bar{Y}\\) of population distributions using a sample of data from that population. The next subsection does this because the mean is not only an important statistic, but because the results can be immediately transferred to the regression context as well.\n\nA.2.1 The sampling distribution of \\(\\bar{Y}\\)\nNow because \\(\\bar{Y}\\) is formed by a sample of \\(\\{Y_i\\}'s\\) it is as well a random variable, and its properties are determined by the sampling distribution of \\(\\bar{Y}\\). Again, we assume that the elements in the sample are drawn at random, that thus the values of \\((Y_1,\\ldots, Y_n)\\) are random, and that thus functions of \\((Y_1,\\ldots, Y_n)\\), such as \\(\\bar{Y}\\), are random: had a different sample been drawn, they would have taken on a different value. Finally, the distribution of \\(\\bar{Y}\\) over different possible samples of size \\(n\\) is called the sampling distribution of \\(\\bar{Y}\\), which underpins all of frequentists statistics.\n\n\nA.2.2 Example: simple binomial random variables\nSo how does this work. Let’s take the easiest statistical example: coin flipping, where the coin is this case is notoriously biased. Suppose the random variable \\(Y\\) takes on 0 (head) or 1 (tails) with the following probability distribution, \\(\\Pr[Y = 0] = 0.22\\), \\(\\Pr(Y =1) = 0.78\\). Then the mean and variance are given by: \\[\\begin{eqnarray}\n\\mu_{Y} &=& p \\times 1 + (1-  p) \\times 0 = p = 0.78 \\notag\\\\\n\\sigma^2_Y&=& E[Y - \\mu_{Y}]^2 = p(1 - p) \\notag\\\\\n&=& 0.78 \\times 0.22 = 0.17\n\\end{eqnarray}\\] But this is only one throw (\\(throw = 1\\)). We would like to have multiple observations to derive at our sampling distribution of \\(\\bar{Y}\\), which we assume to depend on the number of throws, \\(n\\).\nConsider therefore first the case of \\(throw = 2\\). The sampling distribution of \\(\\bar{Y}\\) is, \\[\\begin{eqnarray}\n\\Pr(\\bar{Y}  = 0) &= 0.22^2 &= 0.05 \\notag \\\\\n\\Pr(\\bar{Y}  = 1/2) &=  2 \\times 0.22 \\times 0.78 &= 0.34 \\notag \\\\\n\\Pr(\\bar{Y}  = 1) &= 0.78^2 &= 0.61.\n\\end{eqnarray}\\]\nbut this start to become boring as the number of throws increases. Therefore, we turn to Python. Let’s first check for \\(throw = 2\\).\n\nthrow = 2\nreps = 10000\n\nnp.random.seed(42)\n\n# perform random sampling\nsample_means = []\nfor i in range(reps):\n    coin_2 = np.random.binomial(throw, 0.78)\n    sample_means.append(np.mean(coin_2) / throw)\n\n# Create histogram\nplt.hist(sample_means)\nplt.axvline(x = .78, color = 'red', label = 'Probability tails')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\nFigure A.2: Sampling distribution when you throw a coin 2 times\n\n\n\n\n\nThe first two lines of this code sets the number of throws (throws) and how often I do this (reps). So, I throw a coin twice, for 10000 times in a row. The third line sets a random seed for the random number generator, so that every time that I run this code, I get the same answer. The fourth line of code generates an empty array. The for loop calculate the number of heads, which in this case are no heads (0), head once (1), or two heads (2). To arrive at probabilities I divide by the number of throws again (2) again. Finally, the last four lines gives a histogram of the density with a legend and a vertical line showing the probability to throw a tail.Concerning the number of 42, there is of course nothing special with that number; any number will do. Except that this gives “the answer to the ultimate question of life, the universe, and everything” (Adams 1995).\nBut what if I do this a 1,000 times, so \\(throw = 1000\\)?\n\nthrow = 1000\nreps = 10000\n\n# perform random sampling\nsample_means = []\nfor i in range(reps):\n    coin_100 = np.random.binomial(throw, 0.78)\n    sample_means.append(np.mean(coin_100) / throw)\n\n# Create histogram\nplt.hist(sample_means, bins = 20)\nplt.axvline(x = .78, color = 'red', label = 'Probability tails')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\nFigure A.3: Sampling distribution when you throw a coin a 1,000 times\n\n\n\n\n\nThe histogram can be seen now in Figure A.3.\nBut isn’t this strange. We can now observe a couple of things. First, the average of the distribution of Figure A.3 is very close to 0.78, which is the actual probability that our biased coin provides tails. But, more importantly the distribution starts to look like a symmetric normal distribution. And we started with a binomial distribution!\nThis is the result of two amazing statistical theorems:\n\nThe law of large numbers: the average of the results obtained from a large number of trials should be close to the expected value and tends to become closer to the expected value as more trials are performed. That is, if there a no biases in the experiment itself. It also means that with more experiments the precision become better, or the variance decreases. In general this implies that:\n\n\\(\\bar{Y}\\) is an unbiased estimator of \\(\\mu_Y\\) (that is, \\(E(\\bar{Y}) = \\mu_Y\\))\nvar(\\(\\bar{Y}\\)) is inversely proportional to \\(n\\)\nThe standard error associated with \\(\\bar{Y}\\) is \\(\\sqrt{\\frac{\\sigma_Y^2}{n}}\\) (that means that with larger samples there is less uncertainty but see the square-root law)\n\nThe Central Limit Theorem: when independent random variables are summed up3, their properly normalized sum tends toward a normal distribution even if the original variables themselves are not normally distributed. So \\(\\bar{Y}\\) is approximately distributed \\(N(\\mu_Y,\\frac{\\sigma^2_Y}{n})\\)\n\nWhen working with standardized variables then \\(\\bar{Y} = \\frac{\\bar{Y}-\\mu_Y}{\\sigma_Y/\\sqrt{n}}\\) is approximately distributed as \\(N(0,1)\\) \nThe larger is \\(n\\), the better is the approximation. And this already holds for \\(n \\geq 50\\).4 So with a reasonable amount of observations, the mean of i.i.d. variables is normally distributed\n\n\n\n\n\n\nAdams, Douglas. 1995. Hitchhiker’s Guide to the Galaxy (Book 1). New York: Del Rey Bonks.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Reviewing probability and statistics</span>"
    ]
  },
  {
    "objectID": "appendix.html#footnotes",
    "href": "appendix.html#footnotes",
    "title": "Appendix A — Reviewing probability and statistics",
    "section": "",
    "text": "There is slightly more to this as fractions such as \\(\\frac{1}{2}\\) can in fact be counted as well, and continuous outcomes can be as well complex numbers. But for now we typically see integer numbers as discrete, and real numbers as continuous.↩︎\nThis could be the throw of a dice but as well the measurement of 10,000 house prices.↩︎\nTaking the mean is as well a sum but then divided by a constant.↩︎\nAll applied econometrics assumes the number of observations to be larger than 50.↩︎",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Reviewing probability and statistics</span>"
    ]
  }
]